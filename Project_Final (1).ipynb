{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc48119-e850-49b6-8f4c-5247afd35836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction to Big Data Technologies - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03d11055-342e-46dd-8a06-65c035687947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**by Itay Cohen (211896261) and Tal Skopas (322593070) and Shalev Mahadav (322642752)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2727f1-e259-4727-9900-3a30ab68cda1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "916b15b6-a4e4-4e48-b179-3ba84e7ac35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import ast\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from itertools import accumulate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import (\n",
    "    StringType, DoubleType, LongType,\n",
    "    ArrayType, StructType, StructField, IntegerType\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, dayofmonth, month, year, dayofweek, date_format,\n",
    "    when, lag, udf, collect_list, slice, current_timestamp,\n",
    "    sqrt, pow, unix_timestamp, lit, round as spark_round,\n",
    "    from_json, element_at, get_json_object, split,\n",
    "    trim, row_number, broadcast, avg\n",
    ")\n",
    "\n",
    "df_checkins = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/gowalla_checkins.csv')\n",
    "df_spots1 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/gowalla_spots_subset1.csv').dropDuplicates()\n",
    "df_spots2 = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/gowalla_spots_subset2.csv').dropDuplicates([\"city_state\"])\n",
    "# df_friendship = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/gowalla_friendship.csv')\n",
    "# df_userinfo = spark.read.format('csv').options(header='true', inferSchema='true').load('/FileStore/tables/gowalla_userinfo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802d851d-ab42-49d7-aecb-ba4fd7452833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.4 Clean the data.\n",
    "\n",
    "**For Example:** הסרת רשומות כפולות של אותו המשתמש על אותו מקום בתאריך זהה\n",
    "<div>(3 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae860cb1-ce78-44b8-8283-c7127ec6dbbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "df_checkins = (\n",
    "    df_checkins\n",
    "    .filter(\n",
    "        col(\"datetime\").isNotNull()\n",
    "        & col(\"placeid\").isNotNull() & (col(\"placeid\") > 0)\n",
    "        & (col(\"datetime\") < current_timestamp())\n",
    "    )\n",
    "    .dropDuplicates([\"userid\", \"placeid\", \"datetime\"])\n",
    ")\n",
    "\n",
    "df_spots1 = (\n",
    "    df_spots1\n",
    "    .select(\n",
    "        col(\"id\").alias(\"placeid\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"lng\").cast(\"double\").alias(\"lng\"),\n",
    "        col(\"lat\").cast(\"double\").alias(\"lat\"),\n",
    "        col(\"checkins_count\"),\n",
    "        col(\"spot_categories\"),\n",
    "        col(\"items_count\")\n",
    "    )\n",
    "    .filter(\n",
    "        col(\"lat\").isNotNull()\n",
    "        & col(\"lng\").isNotNull()\n",
    "        & col(\"lat\").between(-90, 90)\n",
    "        & col(\"lng\").between(-180, 180)\n",
    "    )\n",
    ")\n",
    "\n",
    "df_spots2 = (\n",
    "    df_spots2\n",
    "    .select(\n",
    "        col(\"id\").alias(\"placeid\"),\n",
    "        col(\"name\"),\n",
    "        col(\"city_state\"),\n",
    "        col(\"lng\").cast(\"double\").alias(\"lng\"),\n",
    "        col(\"lat\").cast(\"double\").alias(\"lat\")\n",
    "    )\n",
    "    .filter(\n",
    "        col(\"lat\").isNotNull()\n",
    "        & col(\"lng\").isNotNull()\n",
    "        & col(\"lat\").between(-90, 90)\n",
    "        & col(\"lng\").between(-180, 180)\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_spots1)\n",
    "display(df_spots2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "288f5e5d-144d-43eb-be58-ac43a9e58198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Creating a new dataframe that connects between the spots in spots1 to their cities**\n",
    "\n",
    "האלגוריתם מחלק את המפה לריבועים קטנים בגודל של 0.02 מעלות , משייך כל נקודה לריבוע שלה ו24 ריבועים סמוכים, ואז בודק רק את הערים שנמצאות באותם ריבועים בלבד. לאחר מכן הוא מחשב מרחק ישיר בין כל נקודה לכל עיר בסביבה ומסנן החוצה את אלה שמעבר לסף. לבסוף, מתוך כל הערים שעברו את המסנן הוא בוחר לכל נקודה את העיר עם המרחק הקטן ביותר, וכך מוצא עבור כל נקודה את העיר הקרובה מבלי להשוות אותו לעשרות אלפי ערים בכל פעם."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c0f997-a43c-4c1d-8d89-cad2859795c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, floor, sqrt, pow, broadcast, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Prepare df_spots2 – cast lat/lng, filter valid coordinates, keep city info\n",
    "df_spots2_clean = (\n",
    "    df_spots2\n",
    "    # Convert lat/lng to double and filter out nulls or out-of-range values\n",
    "    .withColumn(\"lat2\", col(\"lat\").cast(\"double\"))\n",
    "    .withColumn(\"lng2\", col(\"lng\").cast(\"double\"))\n",
    "    .filter(\n",
    "        (col(\"lat2\").isNotNull()) &\n",
    "        (col(\"lng2\").isNotNull()) &\n",
    "        (col(\"lat2\").between(-90, 90)) &\n",
    "        (col(\"lng2\").between(-180, 180))\n",
    "    )\n",
    "    # Select only the necessary columns\n",
    "    .select(\n",
    "        col(\"placeid\").alias(\"city_id\"),\n",
    "        col(\"city_state\"),\n",
    "        col(\"lat2\"),\n",
    "        col(\"lng2\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare df_spots1 – cast lat/lng and filter valid coordinates\n",
    "df_spots1_clean = (\n",
    "    df_spots1\n",
    "    .withColumn(\"lat1\", col(\"lat\").cast(\"double\"))\n",
    "    .withColumn(\"lng1\", col(\"lng\").cast(\"double\"))\n",
    "    .filter(\n",
    "        (col(\"lat1\").isNotNull()) &\n",
    "        (col(\"lng1\").isNotNull()) &\n",
    "        (col(\"lat1\").between(-90, 90)) &\n",
    "        (col(\"lng1\").between(-180, 180))\n",
    "    )\n",
    "    .select(\n",
    "        col(\"placeid\").alias(\"spot_id\"),\n",
    "        col(\"created_at\"),\n",
    "        col(\"lat1\"),\n",
    "        col(\"lng1\"),\n",
    "        col(\"checkins_count\"),\n",
    "        col(\"items_count\"),\n",
    "        col(\"spot_categories\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define grid-binning: compute a “cell” for each point by truncating lat/lng\n",
    "# We choose 0.02-degree precision. \n",
    "# Then we will only join points whose cells match.\n",
    "\n",
    "# Compute integer bins for lat/lng in both datasets\n",
    "df_spots1_binned = df_spots1_clean.withColumn(\n",
    "    \"lat_bin\", floor(col(\"lat1\") * 50).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"lng_bin\", floor(col(\"lng1\") * 50).cast(\"int\")\n",
    ")\n",
    "\n",
    "df_spots2_binned = df_spots2_clean.withColumn(\n",
    "    \"lat_bin\", floor(col(\"lat2\") * 50).cast(\"int\")\n",
    ").withColumn(\n",
    "    \"lng_bin\", floor(col(\"lng2\") * 50).cast(\"int\")\n",
    ")\n",
    "\n",
    "deltas = [(i, j) for i in (-2, -1, 0, 1, 2) for j in (-2, -1, 0, 1, 2)]\n",
    "deltas_df = spark.createDataFrame(deltas, [\"dlat\", \"dlng\"])\n",
    "\n",
    "\n",
    "spots1_candidates = (\n",
    "    df_spots1_binned\n",
    "    .crossJoin(deltas_df)\n",
    "    .withColumn(\"candidate_lat_bin\", col(\"lat_bin\") + col(\"dlat\"))\n",
    "    .withColumn(\"candidate_lng_bin\", col(\"lng_bin\") + col(\"dlng\"))\n",
    "    .select(\n",
    "        \"spot_id\", \"created_at\", \"lat1\", \"lng1\",\n",
    "        \"checkins_count\", \"items_count\", \"spot_categories\",\n",
    "        \"candidate_lat_bin\", \"candidate_lng_bin\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Join spots1_candidates to cities on matching 25‐cell bins\n",
    "joined_candidates = (\n",
    "    spots1_candidates\n",
    "    .join(\n",
    "        broadcast(df_spots2_binned),\n",
    "        (spots1_candidates.candidate_lat_bin == df_spots2_binned.lat_bin) &\n",
    "        (spots1_candidates.candidate_lng_bin == df_spots2_binned.lng_bin),\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        \"spot_id\", \"created_at\", \"lat1\", \"lng1\",\n",
    "        \"checkins_count\", \"items_count\", \"spot_categories\",\n",
    "        \"city_id\", \"city_state\", \"lat2\", \"lng2\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compute exact distance and filter within 5 km\n",
    "distance_filtered = (\n",
    "    joined_candidates\n",
    "    .withColumn(\n",
    "        \"dist_to_city\",\n",
    "        sqrt(\n",
    "            pow(col(\"lat1\") - col(\"lat2\"), 2) +\n",
    "            pow(col(\"lng1\") - col(\"lng2\"), 2)\n",
    "        )\n",
    "    )\n",
    "    .filter(col(\"dist_to_city\") <= 0.045)\n",
    ")\n",
    "\n",
    "# For each spot, pick the single city with the minimum dist_to_city using a Window\n",
    "w = Window.partitionBy(\"spot_id\").orderBy(col(\"dist_to_city\"))\n",
    "closest_city_per_spot = (\n",
    "    distance_filtered\n",
    "    .withColumn(\"rank\", row_number().over(w))\n",
    "    .filter(col(\"rank\") == 1)\n",
    "    .drop(\"rank\")\n",
    ")\n",
    "\n",
    "# Build final DataFrame of matched spots with city info\n",
    "df_spots_matched = closest_city_per_spot.select(\n",
    "    \"spot_id\",\n",
    "    \"created_at\",\n",
    "    \"lat1\",\n",
    "    \"lng1\",\n",
    "    \"checkins_count\",\n",
    "    \"items_count\",\n",
    "    \"spot_categories\",\n",
    "    \"city_state\",\n",
    "    \"dist_to_city\"\n",
    ")\n",
    "\n",
    "display(df_spots_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e929ff14-0d87-4948-ba00-f278340f5aca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The function extract_categories helps us extract the gencats from the gowalla_category_structure.json file.**\n",
    "\n",
    "For example: the gencat of categories/1 is Food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dce544d-1068-4e82-a360-86dbea76df87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_categories(node, gencat=None):\n",
    "    results = []\n",
    "    stack = [(node, gencat)]\n",
    "\n",
    "    while stack:\n",
    "        current, gencat = stack.pop()\n",
    "        if isinstance(current, list):\n",
    "            for item in reversed(current):\n",
    "                stack.append((item, gencat))\n",
    "        elif isinstance(current, dict):\n",
    "            url = current.get('url')\n",
    "            name = current.get('name')\n",
    "            current_gencat = gencat\n",
    "            if url and name and gencat is None:\n",
    "                current_gencat = name\n",
    "            if url and name:\n",
    "                results.append({\n",
    "                    'url': url,\n",
    "                    'name': name,\n",
    "                    'gencat': current_gencat\n",
    "                })\n",
    "            for key, value in current.items():\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    stack.append((value, current_gencat))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05f87f4f-161c-4968-ab04-ee2df5323d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this need to be added to the preprocess this is the addition of the categories to the dataset\n",
    "# add category\n",
    "\n",
    "results = []\n",
    "categories = spark.read.option(\"multiline\", \"true\").format(\"json\").load(\"/FileStore/tables/gowalla_category_structure.json\")\n",
    "json_cat = json.loads(categories.toJSON().first())\n",
    "results = extract_categories(json_cat[\"spot_categories\"])\n",
    "schema = StructType([\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"gencat\", StringType(), True),\n",
    "])\n",
    "categories_df = spark.createDataFrame(results, schema=schema)\n",
    "\n",
    "category_schema = ArrayType(\n",
    "    StructType([\n",
    "        StructField(\"url\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True)\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Parse the 'categories' column as JSON\n",
    "spots_parsed = df_spots_matched.withColumn(\"parsed_categories\", from_json(col(\"spot_categories\"), category_schema))\n",
    "\n",
    "# Extract the first url from the parsed categories array\n",
    "spots_with_url = spots_parsed.withColumn(\"category_url\", element_at(col(\"parsed_categories.url\"), 1))\n",
    "\n",
    "# Now join on the extracted url\n",
    "df_spots_matched = spots_with_url.join(categories_df, spots_with_url[\"category_url\"] == categories_df[\"url\"], how=\"inner\")\n",
    "\n",
    "display(df_spots_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d92cf72-2d7d-4f28-84a6-d0eda98a2e22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_checkins_with_spots = df_checkins.join(df_spots_matched.select(\n",
    "    col(\"spot_id\").alias(\"placeid\"),\n",
    "    col(\"lat1\"),\n",
    "    col(\"lng1\"),\n",
    "    col(\"gencat\"),\n",
    "    col(\"created_at\").cast(\"timestamp\").alias(\"place_created_at\"),\n",
    "    col(\"city_state\"),\n",
    "    col(\"name\")\n",
    "), on=\"placeid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaee7231-d6fa-4234-ad26-5a907fba92da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.1. Divide datetime into 5 columns - Day, Month, Year, Weekday, DayofTheWeek.\n",
    "\n",
    "**Note:** assume Saturday and Sunday are weekends. Validate the values of the new columns.\n",
    "\n",
    "\n",
    "<div>(3 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de9d21f-f762-4678-b243-877803b5be8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_checkins_with_spots = df_checkins_with_spots.withColumn(\"Day\", dayofmonth(col(\"datetime\"))) \\\n",
    "       .withColumn(\"Month\", month(col(\"datetime\"))) \\\n",
    "       .withColumn(\"Year\", year(col(\"datetime\"))) \\\n",
    "       .withColumn(\"DayOfWeek\", dayofweek(col(\"datetime\"))) \\\n",
    "       .withColumn(\"Weekend\", when(col(\"DayOfWeek\").isin([1,7]), \"כן\").otherwise(\"לא\"))\n",
    "\n",
    "display(df_checkins_with_spots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a27e6ddf-4b9c-4e9e-99e2-7338487ea459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.2. Add last_place_visited column, that calculates last visit date for each user. If he doesn't have any return None.\n",
    "\n",
    "**For example:** אם יש משתמש שביקר בשלושה מקומות שונים\n",
    "בתאריכים 2010-06-28 , 2010-06-24 , 2010-05-01 אז ערכי העמודה יהיו 2010-06-24 , 2010-05-01 , ו -\n",
    "None בהתאמה .\n",
    "\n",
    "\n",
    "<div style=\"color:blue\">(3 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6888b2a8-fcc7-4482-9ca4-44c009fec7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_ordered_window = Window.partitionBy(\"userid\").orderBy(\"datetime\")\n",
    "\n",
    "df_checkins_with_spots = df_checkins_with_spots.withColumn(\"prev_place\", lag(\"placeid\").over(time_ordered_window)) \\\n",
    "       .withColumn(\"last_place_visited\", when(col(\"prev_place\").isNull(), col(\"placeid\")).otherwise(col(\"prev_place\")))\n",
    "\n",
    "display(df_checkins_with_spots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0300972b-065c-4a56-aadb-879b640d875f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3.3 Add last_x_place_visited column, that will calculate the x for every data row, since the last visit of the user.\n",
    "\n",
    "מרחק שעבר מהביקור הקודם, האם המשתמש שינה קטגוריה או עיר מאז הביקור הקודם וכו'. מטרת סעיף זה\n",
    "לייצג את השינוי מאז הביקור הקודם כדי לתמוך בקבלת החלטות של חלוקת המשתמשים לקבוצות על בסיס\n",
    "דפוסי הביקורים שלהם באופן דינמי במערכת.\n",
    "<div>(8 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbca4e6-4a0c-4bec-836d-3d287ed46319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We chose the distance between 2 places column because we figured it might be helpful\n",
    "\n",
    "w = Window.partitionBy(\"userid\").orderBy(\"datetime\")\n",
    "\n",
    "df_checkins_with_spots = df_checkins_with_spots.withColumn(\"last_lat\", lag(\"lat1\").over(w)) \\\n",
    "       .withColumn(\"last_lng\", lag(\"lng1\").over(w)) \\\n",
    "       .withColumn(\"distance_from_last\", \n",
    "                   sqrt(pow(col(\"lat1\") - col(\"last_lat\"), 2) + pow(col(\"lng1\") - col(\"last_lng\"), 2))\n",
    "                  )\n",
    "\n",
    "display(df_checkins_with_spots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0803721f-92a1-409b-8cc2-8be0883d88e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Moreover, we added the column time since last visit hours\n",
    "\n",
    "df_checkins_with_spots = df_checkins_with_spots.withColumn(\"prev_datetime\", lag(\"datetime\").over(time_ordered_window))\n",
    "\n",
    "df_checkins_with_spots = df_checkins_with_spots.withColumn(\n",
    "    \"time_since_last_visit_hours\",\n",
    "    when(col(\"prev_datetime\").isNotNull(),\n",
    "         spark_round((unix_timestamp(\"datetime\") - unix_timestamp(\"prev_datetime\")) / 3600.0, 2)\n",
    "    ).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "display(df_checkins_with_spots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c27488cd-0df3-44cf-a349-4e22cdffd35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Return top-k\n",
    "Given the gencat, year, and k, return the top-k popular places with their popularity measurement\n",
    "<div>(8 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210c0689-cfee-426d-9768-7716963b6c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = df_checkins_with_spots.select(\"Year\", \"placeid\", \"gencat\", \"userid\").rdd\n",
    "\n",
    "category = \"Food\"\n",
    "year_to_check = 2010\n",
    "k = 3\n",
    "\n",
    "top_k = rdd.filter(lambda row: row.gencat == category and row.Year == year).map(lambda row: (row.placeid, row.userid)).map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[-1])\n",
    "\n",
    "is_empty = rdd.isEmpty()\n",
    "print(\"Is RDD empty?\", is_empty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68c6a597-5f79-4720-8f9f-c1f2bed7c5d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Is there long-tail\n",
    "You need to establish if there is a long-tail phenomenon in the food gencat in each of the top 2 cities.          \n",
    "נגדיר תופעת לונג-טייל בעיר ושנה מסוימת,\n",
    "בה מעל 70% מהביקורים בשנה זו נמצאים ב20% מהמקומות הקיימים בעיר זו.\n",
    "<div>(20 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1f9968-1f7f-40aa-ab38-17a14e4f635e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# How we did the match between a place and its city: We distributed our matching calculations by firstly matching \n",
    "# each of the spots with the closest us state than finding the closest city within said state \n",
    "# Than matching all the spots that didn’t correlate with any state with all the non us cities\n",
    "# We grouped the cities by state\n",
    "# Than calculating the centroid of each group, that centroid represents the states location\n",
    "# Than we calculated distance from each centroid and related the spot to a state that its within its borders (by approximating state area)\n",
    "# Than calculated the distance from each city within said state and related the city that said spot is inside its borders (also by approximating)\n",
    "\n",
    "df_with_year = df_spots_matched.withColumn(\"year\", year(col(\"created_at\")))\n",
    "\n",
    "filtered_df = df_with_year.select(\"year\", \"city_state\", \"spot_id\", \"checkins_count\", \"gencat\").withColumn(\"checkins_count\", col(\"checkins_count\").cast(IntegerType())) \\\n",
    "    .where(\n",
    "        (col(\"year\").isNotNull()) &\n",
    "        (col(\"city_state\").isNotNull()) &\n",
    "        (col(\"city_state\") != \"none\") &\n",
    "        (col(\"spot_id\").isNotNull()) &\n",
    "        (col(\"checkins_count\").isNotNull()) &\n",
    "        (col(\"gencat\") == \"Food\")\n",
    "    )\n",
    "\n",
    "# Find top 2 global cities (after dropping nulls)\n",
    "city_checkins_rdd = filtered_df.rdd.map(lambda row: (row[\"city_state\"], row[\"checkins_count\"]))\n",
    "top_2_cities = city_checkins_rdd.reduceByKey(lambda a, b: a + b) \\\n",
    "                                .takeOrdered(2, key=lambda x: -x[1])\n",
    "top_city_names = set([city for city, _ in top_2_cities])\n",
    "\n",
    "# Filter only top cities, group by (year, city, id)\n",
    "spot_rdd = (\n",
    "    filtered_df.rdd\n",
    "    .filter(lambda row: row[\"city_state\"] in top_city_names)\n",
    "    .map(lambda row: ((row[\"year\"], row[\"city_state\"], row[\"spot_id\"]), row[\"checkins_count\"]))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    .map(lambda x: ((x[0][0], x[0][1]), (x[0][2], x[1])))  # ((year, city), (id, checkins))\n",
    "    .groupByKey()\n",
    ")\n",
    "\n",
    "# Analyze long-tail for each (year, city)\n",
    "for (year, city), spot_data in spot_rdd.collect():\n",
    "    spot_list = sorted(list(spot_data), key=lambda x: x[1], reverse=True)\n",
    "    total_checkins = sum([count for _, count in spot_list])\n",
    "\n",
    "    if total_checkins == 0 or len(spot_list) == 0:\n",
    "        continue\n",
    "\n",
    "    # Cumulative check-in percentages\n",
    "    cumulative = list(accumulate([count for _, count in spot_list]))\n",
    "    cumulative_data = list(zip(\n",
    "        [sid for sid, _ in spot_list],\n",
    "        [c for _, c in spot_list],\n",
    "        [c / total_checkins for c in cumulative]\n",
    "    ))\n",
    "\n",
    "    # Determine how many spots needed for 70%\n",
    "    top_spots = [entry for entry in cumulative_data if entry[2] <= 0.7]\n",
    "    if len(top_spots) < len(cumulative_data) and cumulative_data[len(top_spots)][2] > 0.7:\n",
    "        top_spots.append(cumulative_data[len(top_spots)])\n",
    "\n",
    "    top_count = len(top_spots)\n",
    "    total_spots = len(spot_list)\n",
    "    long_tail = (top_count / total_spots) <= 0.2\n",
    "\n",
    "    print(f\"\\n=== Year: {year}, City: {city} ===\")\n",
    "    print(f\"Total check-ins: {total_checkins}\")\n",
    "    print(f\"Total spots: {total_spots}\")\n",
    "    print(f\"Spots for 70% of check-ins: {top_count} ({(top_count / total_spots) * 100:.2f}%)\")\n",
    "    print(f\"Long-tail pattern detected: {long_tail}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93eefe1d-988a-47d1-824d-11b63f4b297f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.1 Create ETL/ELT that creates new user-places structure\n",
    "עליכם להחליט כיצד לייצג באופן יעיל את נתוני המקומות של\n",
    "המשתמשים (טיפ: היעזרו בסעיף 3 ג'). שימו לב ששיקולי אתיקה ופרטיות חשובים ביותר לחברה.\n",
    "<div>(6 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12025986-a9cb-496b-bdc5-cf065957a103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q6_df = df_checkins_with_spots.drop(\"gencat\").drop(\"place_created_at\").drop(\"lng1\").drop(\"lat1\")\n",
    "\n",
    "q6_df = q6_df.na.drop(how=\"any\")\n",
    "\n",
    "user_checkins = q6_df.rdd.map(lambda row: (\n",
    "    row.userid,\n",
    "    (row.datetime, row.name, row.city_state)\n",
    "))\n",
    "\n",
    "user_sorted_checkins = user_checkins.groupByKey().mapValues(\n",
    "    lambda vals: sorted(list(vals), key=lambda x: x[0])\n",
    ")\n",
    "\n",
    "def build_user_history(checkins):\n",
    "    category_counts = defaultdict(int)\n",
    "    total = 0\n",
    "    history = []\n",
    "    for dt, cat, city in checkins:\n",
    "        category_counts[cat] += 1\n",
    "        total += 1\n",
    "        cat_list = sorted(category_counts.items(), key=lambda x: (-x[1], x[0]))\n",
    "        history.append((dt, cat_list.copy(), city, total))\n",
    "    return history\n",
    "\n",
    "user_full_history = user_sorted_checkins.flatMapValues(build_user_history)\n",
    "\n",
    "rdd_result = user_full_history.map(lambda x: (x[0],) + x[1])\n",
    "\n",
    "# print(rdd_result.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec9d1ef5-f374-4450-8cda-ff1e663c71ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6.5 Implement 6.2 and 6.3\n",
    "<div>(15 points)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "841ae70d-6946-4d65-a5bb-ae27062733fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.2\n",
    "\n",
    "def parse_time(dt):\n",
    "    if isinstance(dt, datetime):\n",
    "        return dt\n",
    "    return datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def find_nearest_entry(entries, target_timestamp):\n",
    "    valid_entries = [e for e in entries if parse_time(e[1]) <= target_timestamp]\n",
    "    if not valid_entries:\n",
    "        return None\n",
    "    return max(valid_entries, key=lambda x: parse_time(x[1]))\n",
    "\n",
    "def compute_similarity(target_cats, target_city, other_cats, other_city):\n",
    "    set_target = {cat for cat, _ in target_cats}\n",
    "    set_other = {cat for cat, _ in other_cats}\n",
    "    intersection = len(set_target & set_other)\n",
    "    union = len(set_target | set_other)\n",
    "    jaccard = intersection / union if union != 0 else 0\n",
    "    \n",
    "    location_bonus = 0.5 if (target_city == other_city) else 0\n",
    "    \n",
    "    return jaccard + location_bonus\n",
    "\n",
    "def find_similar_users(rdd, target_user_id, target_timestamp_str, k=5):\n",
    "    target_timestamp = parse_time(target_timestamp_str)\n",
    "    \n",
    "    target_entries = rdd.filter(lambda x: x[0] == target_user_id).collect()\n",
    "    target_entry = find_nearest_entry(target_entries, target_timestamp)\n",
    "    \n",
    "    if not target_entry:\n",
    "        return []\n",
    "    \n",
    "    _, ts, target_cats, target_city, _ = target_entry\n",
    "    \n",
    "    other_users = rdd.filter(lambda x: x[0] != target_user_id) \\\n",
    "        .groupBy(lambda x: x[0]) \\\n",
    "        .mapValues(list) \\\n",
    "        .mapValues(lambda entries: find_nearest_entry(entries, target_timestamp)) \\\n",
    "        .filter(lambda x: x[1] is not None)\n",
    "    \n",
    "    def similarity_mapper(user_entry):\n",
    "        user_id, entry = user_entry\n",
    "        _, _, other_cats, other_city, _ = entry\n",
    "        score = compute_similarity(target_cats, target_city, other_cats, other_city)\n",
    "        return (user_id, score)\n",
    "    \n",
    "    similarities = other_users.map(similarity_mapper)\n",
    "    \n",
    "    return similarities.top(k, key=lambda x: x[1])\n",
    "\n",
    "result = find_similar_users(rdd_result, 34240, '2010-12-25 18:45:00', k=3)\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7efbe684-a620-4fbe-a73d-629cc47ae7a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6.3\n",
    "\n",
    "def parse_time(dt):\n",
    "    if isinstance(dt, datetime):\n",
    "        return dt\n",
    "    return datetime.strptime(dt, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def get_place_info(place_id):\n",
    "    place_row = shalev.filter(shalev.id == place_id).first()\n",
    "    return (place_row.name, place_row.city_state)\n",
    "\n",
    "def calculate_user_score(user_entry, target_gencat, target_city):\n",
    "    categories, last_city, total_visits = user_entry\n",
    "    \n",
    "    target_count = next((count for cat, count in categories if cat == target_gencat), 0)\n",
    "    \n",
    "    score = 0\n",
    "    if last_city == target_city:\n",
    "        score += 50 \n",
    "    score += target_count * 10 \n",
    "    if total_visits > 0:\n",
    "        score += (target_count / total_visits) * 100\n",
    "    \n",
    "    return score\n",
    "\n",
    "def top_k_users_for_place(rdd, place_id, timestamp_str, k):\n",
    "    target_gencat, target_city = get_place_info(place_id)\n",
    "    target_timestamp = parse_time(timestamp_str)\n",
    "    \n",
    "    result = rdd.map(lambda x: (x[0], x[1:])) \\\n",
    "        .groupByKey() \\\n",
    "        .mapValues(lambda vals: max(\n",
    "            [v for v in vals if parse_time(v[0]) <= target_timestamp],\n",
    "            key=lambda x: parse_time(x[0]),\n",
    "            default=None\n",
    "        )) \\\n",
    "        .filter(lambda x: x[1] is not None) \\\n",
    "        .mapValues(lambda x: (\n",
    "            x[1], \n",
    "            x[2], \n",
    "            x[3]\n",
    "        )) \\\n",
    "        .map(lambda x: (\n",
    "            x[0],\n",
    "            calculate_user_score(x[1], target_gencat, target_city)\n",
    "        )) \\\n",
    "        .top(k, key=lambda x: x[1])\n",
    "    \n",
    "    return result\n",
    "\n",
    "result = top_k_users_for_place(rdd_result, 12696, '2011-06-01 12:00:00', k=5)\n",
    "# print(result)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project_Final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}